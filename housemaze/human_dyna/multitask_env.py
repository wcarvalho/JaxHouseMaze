from typing import Optional
import jax
import jax.numpy as jnp
from flax import struct
import distrax

from housemaze import env

TaskRunner = env.TaskRunner
TimeStep = env.TimeStep
StepType = env.StepType

MapInit = env.MapInit


@struct.dataclass
class ResetParams:
    map_init: env.MapInit
    train_objects: jax.Array
    test_objects: jax.Array
    starting_locs: Optional[jax.Array] = None
    curriculum: jax.Array = jnp.array(False)
    label: jax.Array = jnp.array(0)
    randomize_agent: bool = jnp.array(False)


@struct.dataclass
class EnvParams:
    reset_params: ResetParams
    time_limit: int = 100
    p_test_sample_train: float = .5
    force_room: bool = jnp.array(False)
    default_room: bool = jnp.array(0)
    training: bool = True
    terminate_with_done: int = 0  # more relevant for web app
    randomize_agent: bool = False
    randomization_radius: int = 0  # New parameter
    task_probs: jax.Array = None


@struct.dataclass
class EnvState:
    # episode information
    key: jax.Array
    step_num: jax.Array

    # map info
    grid: jax.Array
    agent_pos: jax.Array
    agent_dir: int

    # task info
    map_idx: jax.Array
    task_w: jax.Array
    is_train_task: jax.Array
    task_object: jax.Array
    current_label: jax.Array
    offtask_w: jax.Array
    task_state: Optional[env.TaskState] = None
    successes: Optional[jax.Array] = None


class TimeStep(struct.PyTreeNode):
    state: EnvState

    step_type: StepType
    reward: jax.Array
    discount: jax.Array
    observation: env.Observation
    finished: jax.Array = jnp.array(False)

    def first(self):
        return self.step_type == StepType.FIRST

    def mid(self):
        return self.step_type == StepType.MID

    def last(self):
        return self.step_type == StepType.LAST

def mask_sample(mask, rng):
    # Creating logits based on the mask: -1e8 where mask is 0, 1 where mask is 1
    logits = jnp.where(mask == 1, mask.astype(jnp.float32), -1e8).astype(jnp.float32)

    # Creating the Categorical distribution with the specified logits
    sampler = distrax.Categorical(logits=logits)

    # Splitting the RNG
    rng, rng_ = jax.random.split(rng)

    # Sampling from the distribution
    return sampler.sample(seed=rng_)


def sample_spawn_locs(
    rng, spawn_locs):
    H, W, C = spawn_locs.shape

    spawn_locs = spawn_locs/spawn_locs.sum()
    inner_coords = jax.random.choice(
        key=rng,
        shape=(1,),
        a=jnp.arange(H * W),
        replace=False,
        # Flatten the empty_spaces mask and use it
        # as probability distribution
        p=spawn_locs.flatten()
    )

    # Convert the flattened index to y, x coordinates
    y, x = jnp.divmod(inner_coords[0], W)
    return jnp.array([y, x])

def sample_pos_in_grid(rng, grid, default_pos, radius):
    # CAN PROBABLY REMOVE THIS! NOT USING IT!
    H, W, C = grid.shape

    #def sample_full_grid(rng):
    #    empty_spaces = grid == 0
    #    inner_coords = jax.random.choice(
    #        key=rng,
    #        shape=(1,),
    #        a=jnp.arange(H * W),
    #        replace=False,
    #        # Flatten the empty_spaces mask and use it
    #        # as probability distribution
    #        p=empty_spaces.flatten()
    #    )
        
    #    # Convert the flattened index to y, x coordinates
    #    y, x = jnp.divmod(inner_coords[0], W)
    #    return jnp.array([y, x])

    def sample_within_radius(rng):
        def create_probability_map():
            y, x = jnp.mgrid[0:H, 0:W]
            distance_sq = (y - default_pos[0])**2 + (x - default_pos[1])**2

            # Create a mask for positions within the radius
            within_radius = (distance_sq <= radius**2)

            # Create a mask for empty spaces
            empty_spaces = (grid[:, :, 0] == 0)

            # Combine the masks
            valid_positions = within_radius & empty_spaces

            # Create probability map
            prob_map = valid_positions.astype(jnp.float32)
            prob_map /= prob_map.sum()

            return prob_map

        prob_map = create_probability_map()

        # Flatten the probability map for sampling
        flat_prob_map = prob_map.reshape(-1)

        # Sample a position
        idx = jax.random.choice(
            key=rng,
            a=jnp.arange(flat_prob_map.shape[0]),
            shape=(1,),
            p=flat_prob_map
        )

        # Convert the flat index back to 2D coordinates
        sampled_pos = jnp.unravel_index(idx[0], (H, W))

        return jnp.array(sampled_pos)

    usable_spaces = grid == 0
    return jax.lax.cond(
        radius == 0,
        lambda rng: sample_spawn_locs(rng, usable_spaces),
        sample_within_radius,
        rng
    )

class HouseMaze(env.HouseMaze):

    def total_categories(self, params: EnvParams):
        grid = params.reset_params.map_init.grid
        H, W = grid.shape[-3:-1]
        num_object_categories = self.num_categories
        num_directions = len(env.DIR_TO_VEC)
        num_spatial_positions = H * W
        num_actions = self.num_actions(params) + 1  # including reset action
        return num_object_categories + num_directions + num_spatial_positions + num_actions

    def reset(self, rng: jax.Array, params: EnvParams) -> TimeStep:
        """
        
        1. Sample level.
        """
        ##################
        # sample level
        ##################
        nlevels = len(params.reset_params.curriculum)
        rng, rng_ = jax.random.split(rng)
        reset_params_idx = jax.random.randint(
            rng_, shape=(), minval=0, maxval=nlevels)

        def index(p):
            return jax.lax.dynamic_index_in_dim(
            p, reset_params_idx, keepdims=False)
        reset_params = jax.tree_map(index, params.reset_params)

        grid = reset_params.map_init.grid
        agent_dir = reset_params.map_init.agent_dir

        ##################
        # sample pair
        ##################
        pair_idx = mask_sample(mask=reset_params.train_objects >= 0, rng=rng)

        ##################
        # sample position (function of which pair has been choice)
        ##################
        def sample_pos_from_curriculum(rng_):
            locs = jax.lax.dynamic_index_in_dim(
                reset_params.starting_locs, pair_idx, keepdims=False)
            loc_idx = mask_sample(mask=(locs >= 0).all(-1), rng=rng_)
            loc = jax.lax.dynamic_index_in_dim(
                locs, loc_idx, keepdims=False)
            return loc

        def sample_normal(rng_, reset_params, params):
            return jax.lax.cond(
                jnp.logical_and(reset_params.curriculum, params.training),
                lambda: sample_pos_from_curriculum(rng_),
                lambda: reset_params.map_init.agent_pos
            )

        rng, rng_ = jax.random.split(rng)
        agent_pos = jax.lax.cond(
            jnp.logical_and(params.randomize_agent, reset_params.randomize_agent),
            lambda: sample_spawn_locs(
                rng_, reset_params.map_init.spawn_locs),
            lambda: sample_normal(rng_, reset_params, params)
        )

        ##################
        # sample either train or test object as task object
        ##################
        def index(v, i):
            return jax.lax.dynamic_index_in_dim(v, i, keepdims=False)

        train_object = index(reset_params.train_objects, pair_idx)
        test_object = index(reset_params.test_objects, pair_idx)

        train_object, test_object = jax.lax.cond(
            params.force_room,
            lambda: (index(reset_params.train_objects, params.default_room),
                     index(reset_params.test_objects, params.default_room)),
            lambda: (train_object, test_object)
        )

        def train_sample(rng):
            is_train_task = jnp.array(True)
            return train_object, test_object, is_train_task

        def test_sample(rng):
            is_train_task = jnp.array(False)
            return test_object, train_object, is_train_task

        def train_or_test_sample(rng):

            return jax.lax.cond(
                jax.random.bernoulli(rng, p=params.p_test_sample_train),
                train_sample,
                test_sample,
                rng
            )
        rng, rng_ = jax.random.split(rng)
        task_object, offtask_object, is_train_task = jax.lax.cond(
            params.training,
            train_sample,
            train_or_test_sample,
            rng_
        )

        ##################
        # create task vectors
        ##################
        task_w = self.task_runner.task_vector(task_object)
        offtask_w = self.task_runner.task_vector(offtask_object)
        task_state = self.task_runner.reset(grid, agent_pos)

        ##################
        # create ouputs
        ##################
        state = EnvState(
            key=rng,
            step_num=jnp.asarray(0),
            grid=grid,
            agent_pos=agent_pos,
            agent_dir=agent_dir,
            is_train_task=is_train_task,
            map_idx=reset_params_idx,
            current_label=reset_params.label,
            task_w=task_w,
            task_object=task_object,
            offtask_w=offtask_w,
            task_state=task_state,
        )

        reset_action = jnp.array(self.num_actions() + 1, dtype=jnp.int32)
        timestep = TimeStep(
            state=state,
            step_type=StepType.FIRST,
            reward=jnp.asarray(0.0),
            discount=jnp.asarray(1.0),
            observation=self.make_observation(
                state,
                prev_action=reset_action)
        )
        return timestep

    def step(self, rng: jax.Array, timestep: TimeStep, action: jax.Array, params: EnvParams) -> TimeStep:
        del rng # deterministic function

        if self.action_spec == 'keyboard':
            grid, agent_pos, agent_dir = env.take_action(
                timestep.state.replace(agent_dir=action),
                action=env.MinigridActions.forward)
        elif self.action_spec == 'minigrid':
            grid, agent_pos, agent_dir = env.take_action(
                timestep.state,
                action)
        else:
            raise NotImplementedError(self.action_spec)

        task_state = self.task_runner.step(
            timestep.state.task_state, grid, agent_pos)

        state = timestep.state.replace(
            grid=grid,
            agent_pos=agent_pos,
            agent_dir=agent_dir,
            task_state=task_state,
            step_num=timestep.state.step_num + 1,
        )

        terminated_done = action == self.action_enum().done
        # any object picked up
        terminated_features = (task_state.features > 0).any()
        terminated = jax.lax.switch(
            params.terminate_with_done,
            (
                lambda: terminated_features,
                lambda: terminated_done,
                lambda: terminated_features + terminated_done,
            )
        )
        terminated = terminated >= 1
        task_w = timestep.state.task_w.astype(jnp.float32)
        features = task_state.features.astype(jnp.float32)
        reward = (task_w*features).sum(-1)
        truncated = jnp.equal(state.step_num, params.time_limit)

        step_type = jax.lax.select(
            terminated | truncated, StepType.LAST, StepType.MID)
        discount = jax.lax.select(
            terminated, jnp.asarray(0.0), jnp.asarray(1.0))

        timestep = TimeStep(
            state=state,
            step_type=step_type,
            reward=reward,
            discount=discount,
            observation=self.make_observation(
                state,
                prev_action=action),
        )

        return timestep
